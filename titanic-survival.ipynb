{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-03T17:20:04.903890Z","iopub.execute_input":"2022-06-03T17:20:04.904327Z","iopub.status.idle":"2022-06-03T17:20:04.929943Z","shell.execute_reply.started":"2022-06-03T17:20:04.904229Z","shell.execute_reply":"2022-06-03T17:20:04.929092Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error as mae, confusion_matrix as cm, classification_report, f1_score\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.layers import Dense, Dropout","metadata":{"execution":{"iopub.status.busy":"2022-06-03T17:20:04.931349Z","iopub.execute_input":"2022-06-03T17:20:04.931740Z","iopub.status.idle":"2022-06-03T17:20:12.085014Z","shell.execute_reply.started":"2022-06-03T17:20:04.931711Z","shell.execute_reply":"2022-06-03T17:20:12.084395Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest = pd.read_csv('/kaggle/input/titanic/test.csv')\nsurv = pd.read_csv('/kaggle/input/titanic/gender_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-03T17:20:12.086275Z","iopub.execute_input":"2022-06-03T17:20:12.086527Z","iopub.status.idle":"2022-06-03T17:20:12.114543Z","shell.execute_reply.started":"2022-06-03T17:20:12.086481Z","shell.execute_reply":"2022-06-03T17:20:12.113987Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T17:20:12.116335Z","iopub.execute_input":"2022-06-03T17:20:12.116561Z","iopub.status.idle":"2022-06-03T17:20:12.138157Z","shell.execute_reply.started":"2022-06-03T17:20:12.116534Z","shell.execute_reply":"2022-06-03T17:20:12.137478Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Name, Ticket, and Fare likely don't contribute to survival predictions, and would be difficult to set up numerically for the model. The only reason fare would matter is to compare the relative class of people embarking, but the Pclass field is a much better metric for this purpose.\n\nThe Embarked feature would be interesting to keep, to see if certain regions were better. There are only three possibilities for this field, and so I can one-hot encode these into three new columns. Lastly, the cabin would also likely contribute to the survival rates, were there an easy way to make the data numerical.\n\nI'll start by ignoring cabin for now, and making all the adjustments mentioned above.","metadata":{}},{"cell_type":"code","source":"features = ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch'] # Features to keep\ndata = train.loc[:, features] # New frame with desired features\ndata['Cherbourg'] = (train.Embarked == 'C').astype(int) # One-hot encoding for embarking location\ndata['Queenstown'] = (train.Embarked == 'Q').astype(int)\ndata['Southampton'] = (train.Embarked == 'S').astype(int)\ndata.Sex = data.Sex.map(lambda p: 0 if p == 'male' else 1) # Numerical encoding of male/female\n\ndata.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-03T17:20:12.139120Z","iopub.execute_input":"2022-06-03T17:20:12.139312Z","iopub.status.idle":"2022-06-03T17:20:12.171177Z","shell.execute_reply.started":"2022-06-03T17:20:12.139288Z","shell.execute_reply":"2022-06-03T17:20:12.170348Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"There are still some null entries in the Age column. There is a relatively small amount, I think in the long-term I will try two approaches and compare the results. First will be imputation of missing values. Second, dropping the null rows. Lastly, a regressor to predict age based on available metrics.","metadata":{}},{"cell_type":"code","source":"imputer = SimpleImputer()\ndata_imputed = imputer.fit_transform(data)\ndata_imputed = pd.DataFrame(data_imputed,\n                           columns = data.columns)\n\ndata_dropped = data.dropna(axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T17:20:12.172371Z","iopub.execute_input":"2022-06-03T17:20:12.172590Z","iopub.status.idle":"2022-06-03T17:20:12.182919Z","shell.execute_reply.started":"2022-06-03T17:20:12.172565Z","shell.execute_reply":"2022-06-03T17:20:12.182125Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"I'll take a closer look at the cabin feature to see if it is usable in any capacity:","metadata":{}},{"cell_type":"code","source":"print(train.Cabin.isna().value_counts(), '\\n ~~~~~~~~~')\ntrain.Cabin.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-03T17:20:12.184393Z","iopub.execute_input":"2022-06-03T17:20:12.184645Z","iopub.status.idle":"2022-06-03T17:20:12.200218Z","shell.execute_reply.started":"2022-06-03T17:20:12.184616Z","shell.execute_reply":"2022-06-03T17:20:12.199578Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"I'd first like to check if there is any correlation between the deck of the passenger, and the Pclass feature. To do this, I will strip the deck of the first Cabin string for each row, and use this to compare agains the passenger class.","metadata":{}},{"cell_type":"code","source":"deck = train.Cabin.astype(str).map(lambda p: p.split(' ')[0][0]) # Strip the deck from the string\nnon_na_cabins = deck != 'n' # If there are any passengers who were split between multiple decks, this will not account for that.\ndeck.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-03T17:20:12.201389Z","iopub.execute_input":"2022-06-03T17:20:12.201803Z","iopub.status.idle":"2022-06-03T17:20:12.216868Z","shell.execute_reply.started":"2022-06-03T17:20:12.201772Z","shell.execute_reply":"2022-06-03T17:20:12.215961Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"corr_data = pd.DataFrame() # Empty frame to use for correlation data\n\nfor Pclass in [1,2,3]: # Iterate over the three passenger classes\n    corr_data[f'Class {str(Pclass)}'] = (train.Pclass.loc[non_na_cabins] == Pclass).astype(int) # One-hot encode the data\nfor deck_ in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'T']:  # Iterate over possible decks\n    corr_data[f'Deck {deck_}'] = (deck == deck_).astype(int) # One-hot encode the data\n\ncorr = corr_data.corr() # Plot the correlation matrix and do some formatting\nax = sns.heatmap(corr)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45);","metadata":{"execution":{"iopub.status.busy":"2022-06-03T17:20:12.219205Z","iopub.execute_input":"2022-06-03T17:20:12.219859Z","iopub.status.idle":"2022-06-03T17:20:12.552639Z","shell.execute_reply.started":"2022-06-03T17:20:12.219819Z","shell.execute_reply":"2022-06-03T17:20:12.551885Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"It looks like there is some correlation, though it is not discrete. First class passengers were more often situated around decks A, B, and C. Second class were on decks D, E, and F. Lastly, third class were on decks E, F, and G. Deck T was likely a technical crew deck due to the very low passenger count, and the large jump in alphabetical order (A quick Google search confirms that the T deck was used for engine and boiler crew). There is definitely some dependence between the deck and the passenger class, however, I am unsure whether removing one or the other would be beneficial.\n\nDue to the large number of missing cabin information, I will elect to drop the Cabin feature entirely. Nearly 80% of the data is missing a Cabin value. Combined with the relative correlation between Deck and Pclass, I feel this is a safe course of action. Since the Cabin feature was not pulled into the above data earlier, no extra work is necessary at this point.","metadata":{}},{"cell_type":"code","source":"data_imputed.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-03T17:20:12.554784Z","iopub.execute_input":"2022-06-03T17:20:12.555012Z","iopub.status.idle":"2022-06-03T17:20:12.570395Z","shell.execute_reply.started":"2022-06-03T17:20:12.554984Z","shell.execute_reply":"2022-06-03T17:20:12.569506Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"At this point, the imputed data contains all numerical values, no missing or NaN. The last step is to scale the data for use in the training model.","metadata":{}},{"cell_type":"code","source":"print((data_imputed < 0).any()) # Nothing is negative, that's good","metadata":{"execution":{"iopub.status.busy":"2022-06-03T17:20:12.571619Z","iopub.execute_input":"2022-06-03T17:20:12.571938Z","iopub.status.idle":"2022-06-03T17:20:12.585291Z","shell.execute_reply.started":"2022-06-03T17:20:12.571907Z","shell.execute_reply":"2022-06-03T17:20:12.584378Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"print((data_dropped < 0).any()) # Nothing is negative, that's good","metadata":{"execution":{"iopub.status.busy":"2022-06-03T17:20:12.586559Z","iopub.execute_input":"2022-06-03T17:20:12.586859Z","iopub.status.idle":"2022-06-03T17:20:12.597968Z","shell.execute_reply.started":"2022-06-03T17:20:12.586820Z","shell.execute_reply":"2022-06-03T17:20:12.597197Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Using two separate scalers for each data approach:","metadata":{}},{"cell_type":"code","source":"imputed_scaler = MinMaxScaler()\ndata_imp_scl = pd.DataFrame(imputed_scaler.fit_transform(data_imputed))\ndata_imp_scl.columns = data_imputed.columns\n\ndropped_scaler = MinMaxScaler()\ndata_drp_scl = pd.DataFrame(dropped_scaler.fit_transform(data_dropped))\ndata_drp_scl.columns = data_dropped.columns\n","metadata":{"execution":{"iopub.status.busy":"2022-06-03T17:20:12.599113Z","iopub.execute_input":"2022-06-03T17:20:12.599519Z","iopub.status.idle":"2022-06-03T17:20:12.617280Z","shell.execute_reply.started":"2022-06-03T17:20:12.599490Z","shell.execute_reply":"2022-06-03T17:20:12.616285Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch']\n\n\nX_imp = data_imp_scl[features]\ny_imp = data_imp_scl['Survived']\nX_dr = data_drp_scl[features]\ny_dr = data_drp_scl['Survived']\n\"\"\"\nX_imp = data_imputed[features]\ny_imp = data_imputed['Survived']\nX_dr = data_dropped[features]\ny_dr = data_dropped['Survived']\n\"\"\"\nX_tr_imp , X_te_imp, y_tr_imp, y_te_imp = train_test_split(X_imp, y_imp)\nX_tr_drp , X_te_drp, y_tr_drp, y_te_drp = train_test_split(X_dr, y_dr)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-03T17:20:12.618523Z","iopub.execute_input":"2022-06-03T17:20:12.619239Z","iopub.status.idle":"2022-06-03T17:20:12.629360Z","shell.execute_reply.started":"2022-06-03T17:20:12.619197Z","shell.execute_reply":"2022-06-03T17:20:12.628675Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(X_tr_imp.shape)\nprint(y_tr_imp.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T17:20:12.630227Z","iopub.execute_input":"2022-06-03T17:20:12.630692Z","iopub.status.idle":"2022-06-03T17:20:12.644715Z","shell.execute_reply.started":"2022-06-03T17:20:12.630644Z","shell.execute_reply":"2022-06-03T17:20:12.643700Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n\ndef eval_model(model, X_tr, X_te, y_tr, y_te):\n    model.fit(X_tr, y_tr)\n    y_pr = model.predict(X_te).astype(int)\n    print(classification_report(y_te, y_pr, zero_division=0))\n    return\n\nmodels = {'KNN': KNeighborsClassifier(), \n          'CategoricalNB': CategoricalNB(), \n          'Radius Neighbors': RadiusNeighborsClassifier(radius=2), \n          'SGD Classifier': SGDClassifier(),\n          'Support Vector Machine': SVC(),\n          'Gaussian Classifier': GaussianProcessClassifier(),\n          'Decision Tree Classifier': DecisionTreeClassifier()}\n\nfor model in models.keys():\n    print(model)\n    np.random.seed(31415) # Set the random seed every time for reproducible results to compare\n    eval_model(models[model], X_tr_imp, X_te_imp, y_tr_imp, y_te_imp)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-03T17:20:12.645637Z","iopub.execute_input":"2022-06-03T17:20:12.646205Z","iopub.status.idle":"2022-06-03T17:20:13.319529Z","shell.execute_reply.started":"2022-06-03T17:20:12.646177Z","shell.execute_reply":"2022-06-03T17:20:13.318805Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"The Decision Tree, KNN, and SVC algorithms performed in the top bracket based on F1 score. I will pick these three to move forward. Next I want to compare the performance of imputation versus dropping, as well as the performance of StandardScaler vs MinMaxScaler.","metadata":{}},{"cell_type":"code","source":"models = {'KNN' : KNeighborsClassifier(),\n          'Tree': DecisionTreeClassifier(),\n          'SVC' : SVC()}\n\nprint('Dropped Values')\nfor model in models.keys():\n    print(model)\n    np.random.seed(31415)\n    eval_model(models[model], X_tr_drp, X_te_drp, y_tr_drp,  y_te_drp)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-03T17:20:13.321273Z","iopub.execute_input":"2022-06-03T17:20:13.322595Z","iopub.status.idle":"2022-06-03T17:20:13.408649Z","shell.execute_reply.started":"2022-06-03T17:20:13.322553Z","shell.execute_reply":"2022-06-03T17:20:13.408088Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"When compared to the above results, the dropped values perform strongly ahead of the imputed values, particularly for the Decision Tree and KNN algorithms. Recall is still relatively low, at around 70% for both algorithms, but they both have a high precision ~90%. I'll set my focus on these two algorithms, starting with the KNN algorithm.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nknn_model = KNeighborsClassifier()\n\nparams = {\n    'n_neighbors' : np.arange(5, 101, 5),\n    'weights' : ['uniform', 'distance'],\n    'algorithm' : ['kd_tree', 'ball_tree', 'brute'],\n    'leaf_size' : np.arange(10, 151, 5),\n    'p' : [1,2,3]\n}\n\nclf = RandomizedSearchCV(knn_model, params, n_iter=1000, cv = 2, random_state = 31415)\nmodel = clf.fit(X_tr_drp, y_tr_drp)\n\ny_pr_drp = model.predict(X_te_drp)\nprint(classification_report(y_te_drp, y_pr_drp))","metadata":{"execution":{"iopub.status.busy":"2022-06-03T17:20:13.409577Z","iopub.execute_input":"2022-06-03T17:20:13.409879Z","iopub.status.idle":"2022-06-03T17:20:44.256408Z","shell.execute_reply.started":"2022-06-03T17:20:13.409853Z","shell.execute_reply":"2022-06-03T17:20:44.255873Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\ndtc_model = DecisionTreeClassifier()\n\nparams = {\n    'criterion' : ['gini', 'entropy'],\n    'splitter' : ['best', 'random'],\n    'max_leaf_nodes' : np.arange(10, 1001, 10)\n}\n\nclf = GridSearchCV(dtc_model, params, cv = 2)\nmodel = clf.fit(X_tr_drp, y_tr_drp)\n\ny_pr_drp = model.predict(X_te_drp)\nprint(classification_report(y_te_drp, y_pr_drp))","metadata":{"execution":{"iopub.status.busy":"2022-06-03T17:20:44.257324Z","iopub.execute_input":"2022-06-03T17:20:44.257608Z","iopub.status.idle":"2022-06-03T17:20:47.285614Z","shell.execute_reply.started":"2022-06-03T17:20:44.257582Z","shell.execute_reply":"2022-06-03T17:20:47.284970Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Once again the two algorithms are neck-and-neck. Surprisingly, the intial parameter search didn't significantly improve the results, and so this problem may be out of my expertise to optimize.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}